{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Webscraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import requests as req\n",
    "\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from urllib.parse import urljoin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple request to retrieve webpage title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.scwcsu.nhs.uk/about/our-values'\n",
    "\n",
    "response = req.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    print('Webpage title:', soup.title.string)\n",
    "\n",
    "else:\n",
    "    print(f'Failed to fetch webpage: {response.status_code}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the HTML for a page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.prettify()) # i.e. accessing the BeautifulSoup object created above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Locate a .csv file on a webpage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url2 = ('https://digital.nhs.uk/data-and-information/publications/statistical/out-of-area-placements-in-mental-health-services/march-2024') \n",
    "\n",
    "response2 = req.get(url2)\n",
    "\n",
    "if response2.status_code == 200:\n",
    "    soup2 = BeautifulSoup(response2.content, 'html.parser')\n",
    "    csv_link = soup2.find(\"a\", href=lambda href: href and href.endswith('csv'))\n",
    "    file_url = csv_link[\"href\"]\n",
    "    print(\"Found .csv file:\", file_url)\n",
    "\n",
    "else:\n",
    "    print(f'Failed to fetch webpage: {response2.status_code}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the response and download the file to the current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = file_url.split(\"/\")[-1]  # Extract the file name from the URL i.e. the bit after the last \"/\"\n",
    "file_response = req.get(file_url)\n",
    "\n",
    "if file_response.status_code == 200:\n",
    "    # Save the file to the current directory\n",
    "    with open(f'{file_name}', \"wb\") as file:\n",
    "        file.write(file_response.content)\n",
    "    print(f\"Downloaded: {file_name}\")\n",
    "else:\n",
    "    print(f\"Failed to download: {file_url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the .csv directly into a Pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO                     # io is native to Python\n",
    "\n",
    "csv_content = StringIO(file_response.text)  # convert the context to a file-like object\n",
    "\n",
    "df = pd.read_csv(csv_content)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download multiple files of a particular type from a webpage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url3 = ('https://www.england.nhs.uk/statistics/statistical-work-areas/serious-mental-illness-smi/')\n",
    "\n",
    "response3 = req.get(url3)\n",
    "\n",
    "if response3.status_code == 200:\n",
    "    soup3 = BeautifulSoup(response3.content, \"html.parser\")\n",
    "    \n",
    "  \n",
    "    for link in soup3.find_all(\"a\", href=True):\n",
    "        file_url = link[\"href\"]\n",
    "        if file_url.endswith(('.xlsx')):  # Check for .xlsx file extensions\n",
    "            print(\"Found .xlsx file:\", file_url)\n",
    "            \n",
    "            # Step 4: Download the file\n",
    "            file_name = file_url.split(\"/\")[-1]  # Extract the file name from the URL i.e. everything after the last /\n",
    "            file_response = req.get(file_url)\n",
    "            \n",
    "            if file_response.status_code == 200:\n",
    "                # Save the file to the current directory\n",
    "                with open(file_name, \"wb\") as file:\n",
    "                    file.write(file_response.content)\n",
    "                print(f\"Downloaded: {file_name}\")\n",
    "            else:\n",
    "                print(f\"Failed to download: {file_url}\")\n",
    "    \n",
    "else:\n",
    "    print(f\"Failed to fetch webpage: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape information from a table on a webpage\n",
    "\n",
    "For example, if you wanted to scrape a publication schedule, or in this case a submission schedule for Providers' MHSDS data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url4 = ('https://digital.nhs.uk/data-and-information/data-collections-and-data-sets/data-sets/mental-health-services-data-set/submit-data')\n",
    "\n",
    "response4 = req.get(url4)\n",
    "\n",
    "if response4.status_code == 200:\n",
    "    soup4 = BeautifulSoup(response4.content, \"html.parser\")\n",
    "\n",
    "tables = soup4.find_all('table') # \n",
    "print(type(tables))\n",
    "\n",
    "table_df = pd.read_html(str(tables))[0] # get the first item in the beautifulsoup ResultSet, convert it into a string, and read the html into a pandas DataFrame\n",
    "\n",
    "table_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doesn't appear to cover the whole year. Is there another section to the table?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pd.read_html((str(tables)))) # check how many individual tables have been found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_df2 = pd.read_html(str(tables))[1] # return the second item from the ResultSet\n",
    "\n",
    "\n",
    "table_df2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using urljoin to construct URLs\n",
    "\n",
    "This will extract any .csv files for the calendar year 2024, which are all saved to their individual web pages, meaning that urljoin is required to construct the URLs dynamically (i.e. so that you don't have to hard code all the indidual web pages).\n",
    "\n",
    "The package \"re\" is imported so that regular expression logic can be used in the construction of the URLs i.e. anything matching the patterm of the regular expression will be considered a web page of interest. (NOTE: you do not need to install \"re\", it is native to Python)\n",
    "\n",
    "It's been limited to 2024 files to reduce the amount of data being transferred, but you could use a different regular expression to cover more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urljoin\n",
    "import re\n",
    "\n",
    "url5 = 'https://digital.nhs.uk/data-and-information/publications/statistical/learning-disabilities-health-check-scheme'\n",
    "\n",
    "target_urls = []                           # empty list that will later get filled with target URLs in a for loop.\n",
    "\n",
    "dynamic_section = r'^england-[a-z]+-2024$' # the regular expression for the URLs we are interested in. note that the $ implies that you don't want anything else to follow.\n",
    "\n",
    "response5 = req.get(url5)                  # get the response from the base URL\n",
    "\n",
    "if response5.status_code == 200:\n",
    "    soup5 = BeautifulSoup(response5.content, \"html.parser\")     # if there is a successful response, create a BeautifulSoup object.\n",
    "\n",
    "    for link in soup5.find_all('a', href = True):\n",
    "        sublink = link[\"href\"]\n",
    "        if re.match(dynamic_section,sublink.split('/')[-1]):\n",
    "            full_url = urljoin(url5, sublink)                   # for each of the instances of the pattern we are looking for\n",
    "            target_urls.append(full_url)                        # add the constructed full URL to a list of target URLs\n",
    "        \n",
    "    for link in target_urls:                                    # check for a successful response (code 200) from each URL\n",
    "        response = req.get(link)                                # and create a BeautifulSoup object for each.\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "            for link in soup.find_all(\"a\", href=True):          # for each URL found on each of the pages in target_urls...\n",
    "                file_url = link['href']                         \n",
    "\n",
    "                if file_url.endswith(('.csv')):                 # ... check for .csv file extensions\n",
    "                    print(\"Found .csv file:\", file_url)\n",
    "\n",
    "                    file_name = file_url.split(\"/\")[-1]         # extract the file name from the URL i.e. everything after the last /\n",
    "                    file_response = req.get(file_url)           # check the response for each file\n",
    "            \n",
    "                    if file_response.status_code == 200:        # if there's a successful response\n",
    "                        \n",
    "                        with open(file_name, \"wb\") as file:     # save the file to the current directory\n",
    "                            file.write(file_response.content)\n",
    "                        print(f\"Downloaded: {file_name}\")\n",
    "                    else:\n",
    "                        print(f\"Failed to download: {file_url}\")\n",
    "\n",
    "else:\n",
    "    print(f'Failed to fetch webpage: {response.status_code}')   # this else statement pairs with the original response code check for the base URL\n",
    "                                                                # (see the first \"if\" in this code block)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "surgery-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
